{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from watch_recognition.utilities import BBox\n",
    "\n",
    "\n",
    "from PIL.Image import BICUBIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kp_image_size = (224, 224)\n",
    "classes = [\"clock-face\"]\n",
    "COLORS = [(255, 0, 255)]\n",
    "detection_model = (\n",
    "    \"./models/detection/efficientdet_lite0/run_1633100188.371347/model.tflite\"\n",
    ")\n",
    "\n",
    "temp_file = \"/tmp/test-image.png\"\n",
    "detector = tf.lite.Interpreter(model_path=detection_model)\n",
    "detector.allocate_tensors()\n",
    "# 'models/keypoint/efficientnetb0/run_1633543680.356453.h5'\n",
    "\n",
    "\n",
    "keypoint_model = 'models/keypoint/efficientnetb0-unet-sigmoid/run_1635001687.225322'\n",
    "kp_model = tf.keras.models.load_model(keypoint_model, compile=False)\n",
    "rotation_model = (\n",
    "    \"./rotation-model-cls-4-fc-4.h5\"\n",
    ")\n",
    "rotation_model = tf.keras.models.load_model(rotation_model, compile=False)\n",
    "frames_dir = Path(\"./frames/\")\n",
    "frames_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lru_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from watch_recognition.utilities import Point\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "    \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "    # Feed the input image to the model\n",
    "    set_input_tensor(interpreter, image)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get all outputs from the model\n",
    "    boxes = get_output_tensor(interpreter, 0)\n",
    "    classes = get_output_tensor(interpreter, 1)\n",
    "    scores = get_output_tensor(interpreter, 2)\n",
    "    count = int(get_output_tensor(interpreter, 3))\n",
    "\n",
    "    results = []\n",
    "    for i in range(count):\n",
    "        if scores[i] >= threshold:\n",
    "            result = {\n",
    "                \"bounding_box\": boxes[i],\n",
    "                \"class_id\": classes[i],\n",
    "                \"score\": scores[i],\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "# functions to run object detector in tflite from object detector model maker\n",
    "\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "    \"\"\"Set the input tensor.\"\"\"\n",
    "    tensor_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "    \"\"\"Retur the output tensor at the given index.\"\"\"\n",
    "    output_details = interpreter.get_output_details()[index]\n",
    "    tensor = np.squeeze(interpreter.get_tensor(output_details[\"index\"]))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "    \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "    original_image = img\n",
    "    resized_img = tf.image.resize(img, input_size)\n",
    "    resized_img = resized_img[tf.newaxis, :]\n",
    "    return resized_img, original_image\n",
    "\n",
    "def _run_detector_tflite(image_path) -> List[BBox]:\n",
    "    \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "    if isinstance(image_path, (str, Path)):\n",
    "        im = Image.open(image_path)\n",
    "    elif isinstance(image_path, np.ndarray):\n",
    "        im = Image.fromarray(image_path)\n",
    "    else:\n",
    "        raise ValueError(f\"unrecognized image type {type(image_path)}\")\n",
    "    im.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "    # TODO skip temp file?\n",
    "    im.save(temp_file, \"PNG\")\n",
    "\n",
    "    # Load the input shape required by the model\n",
    "    _, input_height, input_width, _ = detector.get_input_details()[0][\"shape\"]\n",
    "\n",
    "    # Load the input image and preprocess it\n",
    "    preprocessed_image, original_image = preprocess_image(\n",
    "        temp_file, (input_height, input_width)\n",
    "    )\n",
    "\n",
    "    # Run object detection on the input image\n",
    "    results = detect_objects(detector, preprocessed_image, threshold=0.5\n",
    "                             )\n",
    "\n",
    "    bboxes = []\n",
    "    for obj in results:\n",
    "        ymin, xmin, ymax, xmax = obj[\"bounding_box\"]\n",
    "        # Find the class index of the current object\n",
    "        class_id = int(obj[\"class_id\"])\n",
    "        class_name = classes[class_id]\n",
    "        score = float(obj[\"score\"])\n",
    "        bboxes.append(\n",
    "            BBox(\n",
    "                x_min=xmin,\n",
    "                y_min=ymin,\n",
    "                x_max=xmax,\n",
    "                y_max=ymax,\n",
    "                name=class_name,\n",
    "                score=score,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def plot_detection_on_image(original_image, results: List[BBox]):\n",
    "    # Plot the detection results on the input image\n",
    "    original_image_np = original_image.astype(np.uint8)\n",
    "    for obj in results:\n",
    "        # Convert the object bounding box from relative coordinates to absolute\n",
    "        # coordinates based on the original image resolution\n",
    "        xmin, ymin, xmax, ymax = obj.as_coordinates_tuple\n",
    "        xmin = int(xmin * original_image_np.shape[1])\n",
    "        xmax = int(xmax * original_image_np.shape[1])\n",
    "        ymin = int(ymin * original_image_np.shape[0])\n",
    "        ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "        # Draw the bounding box and label on the image\n",
    "        color = [int(c) for c in COLORS[0]]\n",
    "        cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "        # Make adjustments to make the label visible for all objects\n",
    "        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "        label = obj.name\n",
    "        cv2.putText(original_image_np, label, (xmin, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Return the final image\n",
    "    original_uint8 = original_image_np.astype(np.uint8)\n",
    "    return original_uint8\n",
    "\n",
    "\n",
    "def plot_kp_on_image(original_image, point: Point):\n",
    "    # Plot the detection results on the input image\n",
    "    original_image_np = original_image.astype(np.uint8)\n",
    "\n",
    "    x, y = point.as_coordinates_tuple\n",
    "    x = int(x * original_image_np.shape[1])\n",
    "    y = int(y * original_image_np.shape[0])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    cv2.drawMarker(original_image_np, (x, y), (255, 0, 0), cv2.MARKER_CROSS, thickness=3)\n",
    "\n",
    "    # Return the final image\n",
    "    original_uint8 = original_image_np.astype(np.uint8)\n",
    "    return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import ImageOps\n",
    "from watch_recognition.targets_encoding import convert_mask_outputs_to_keypoints\n",
    "from skimage.transform import rotate\n",
    "from tqdm import tqdm\n",
    "import dataclasses\n",
    "from watch_recognition.models import points_to_time\n",
    "\n",
    "file = Path('../IMG_1200.MOV')\n",
    "assert file.exists()\n",
    "cap = cv2.VideoCapture(str(file))\n",
    "\n",
    "# Read until video is completed\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "out = cv2.VideoWriter(\n",
    "    \"outpy_12.mov\",\n",
    "    cv2.VideoWriter_fourcc(\"M\", \"J\", \"P\", \"G\"),\n",
    "    cap.get(cv2.CAP_PROP_FPS) - 10,\n",
    "    (1080, 1080),\n",
    ")\n",
    "use_angle_model = False\n",
    "frame_id = 0\n",
    "# TODO profile this loop and make it faster\n",
    "# TODO can we know upfront how many frames are there?\n",
    "with tqdm(total=393) as pbar:\n",
    "    while cap.isOpened():\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # TODO crop rectangle from the center of the frame\n",
    "\n",
    "            x = frame.shape[1]\n",
    "            y = frame.shape[0]\n",
    "            x_min = int((y - x) / 2)\n",
    "            frame = frame[x_min - 100 : -x_min - 100, :, :]\n",
    "            if frame_id in detections_cache:\n",
    "                results = detections_cache[frame_id]\n",
    "            else:\n",
    "                results = _run_detector_tflite(frame)\n",
    "                detections_cache[frame_id] = results\n",
    "            results = sorted(results, key=lambda x: x.x_min)\n",
    "            new_results = []\n",
    "            with Image.fromarray(frame) as pil_img:\n",
    "                for i, bbox in enumerate(results):\n",
    "                    try:\n",
    "                        scaled_bbox = bbox.scale(pil_img.width, pil_img.height)\n",
    "                        max_dim = max(scaled_bbox.width, scaled_bbox.height)\n",
    "                        scaled_bbox = scaled_bbox.center_scale(\n",
    "                                max_dim / scaled_bbox.width,\n",
    "                                max_dim / scaled_bbox.height,\n",
    "                        )\n",
    "                        crop_box = tuple(map(int, scaled_bbox.as_coordinates_tuple))\n",
    "\n",
    "                        crop = pil_img.crop(crop_box)\n",
    "                        crop.save(frames_dir / f\"{frame_id}_{i}.jpg\")\n",
    "                        # rotation\n",
    "                        crop_resized = ImageOps.pad(\n",
    "                            crop,\n",
    "                            tuple(kp_model.inputs[0].shape[1:3]), BICUBIC\n",
    "                        )\n",
    "                        # crop_resized = crop.resize(\n",
    "                        #     rotation_model.inputs[0].shape[1:3], BICUBIC\n",
    "                        # )\n",
    "                        crop_resized_np = tf.keras.preprocessing.image.img_to_array(\n",
    "                            crop_resized\n",
    "                        )\n",
    "                        pred_angle = 0\n",
    "                        if use_angle_model:\n",
    "                            pred_angle = rotation_model.predict(\n",
    "                                np.expand_dims(crop_resized_np, 0)\n",
    "                            )\n",
    "                            pred_angle = pred_angle.argmax(axis=1) * 90\n",
    "                            pred_angle = pred_angle[0]\n",
    "\n",
    "                            if pred_angle:\n",
    "                                crop_np = tf.keras.preprocessing.image.img_to_array(crop)\n",
    "                                rotated_crop = rotate(crop_np, -pred_angle).astype(\n",
    "                                    \"float32\"\n",
    "                                )\n",
    "                                rotated_crop = tf.keras.preprocessing.image.array_to_img(\n",
    "                                    rotated_crop\n",
    "                                )\n",
    "                                crop_rotated_resized = rotated_crop.resize(\n",
    "                                    kp_model.inputs[0].shape[1:3], BICUBIC\n",
    "                                )\n",
    "                                crop_rotated_resized_np = (\n",
    "                                    tf.keras.preprocessing.image.img_to_array(\n",
    "                                        crop_rotated_resized\n",
    "                                    )\n",
    "                                )\n",
    "                            else:\n",
    "                                crop_rotated_resized_np = crop.resize(\n",
    "                                    kp_model.inputs[0].shape[1:3], BICUBIC\n",
    "                                )\n",
    "                        else:\n",
    "                            crop_rotated_resized_np = crop.resize(\n",
    "                                    kp_model.inputs[0].shape[1:3], BICUBIC\n",
    "                                )\n",
    "\n",
    "                        # keypoints\n",
    "                        predicted = kp_model.predict(\n",
    "                            np.expand_dims(crop_rotated_resized_np, 0)\n",
    "                        )[0]\n",
    "                        scale_x = crop.width / predicted.shape[0]\n",
    "                        scale_y = crop.height / predicted.shape[1]\n",
    "                        outputs = convert_mask_outputs_to_keypoints(predicted)\n",
    "                        if use_angle_model:\n",
    "                            print(outputs)\n",
    "                            crop_keypoints = [\n",
    "                                p.rotate_around_origin_point(\n",
    "                                    Point(predicted.shape[0] / 2, predicted.shape[1] / 2),\n",
    "                                    -pred_angle,\n",
    "                                )\n",
    "                                .scale(scale_x, scale_y)\n",
    "                                .translate(scaled_bbox.x_min, scaled_bbox.y_min)\n",
    "                                .scale(1 / pil_img.width, 1 / pil_img.height)\n",
    "                                for p in outputs\n",
    "                            ]\n",
    "                        else:\n",
    "                            crop_keypoints = [\n",
    "                                p\n",
    "                                .scale(scale_x, scale_y)\n",
    "                                .translate(scaled_bbox.x_min, scaled_bbox.y_min)\n",
    "                                .scale(1 / pil_img.width, 1 / pil_img.height)\n",
    "                                for p in outputs\n",
    "                            ]\n",
    "                        for kp in crop_keypoints:\n",
    "                            frame = plot_kp_on_image(frame, kp)\n",
    "                        mean_score = int(\n",
    "                            np.round(np.mean([o.score for o in outputs]), 2) * 100\n",
    "                        )\n",
    "                        center, top, hour, minute = [\n",
    "                            np.array(p.as_coordinates_tuple).astype(float)\n",
    "                            for p in outputs\n",
    "                        ]\n",
    "                        read_hour, read_minute = points_to_time(\n",
    "                            center, hour, minute, top\n",
    "                        )\n",
    "                        if use_angle_model:\n",
    "                            time = f\"{read_hour:.0f}:{read_minute:.0f}[{int(pred_angle)}]\"\n",
    "                        else:\n",
    "                            time = f\"{read_hour:.0f}:{read_minute:.0f}\"\n",
    "\n",
    "                        new_results.append(dataclasses.replace(bbox, name=time))\n",
    "                    except Exception as e:\n",
    "                        print(e, frame_id)\n",
    "                        raise e\n",
    "            frame = plot_detection_on_image(frame, new_results)\n",
    "            #             plt.imshow(frame)\n",
    "            # Write the frame into the file 'output.avi'\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            out.write(frame)\n",
    "        # Break the loop\n",
    "        else:\n",
    "            break\n",
    "        frame_id += 1\n",
    "        pbar.update(1)\n",
    "        # break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "frame_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-watch-metal)",
   "language": "python",
   "name": "tf-watch-metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
