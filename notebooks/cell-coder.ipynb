{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from watch_recognition.models import build_backbone\n",
    "from watch_recognition.reports import log_scalar_metrics, run_on_image_debug, generate_report_for_keypoints\n",
    "from watch_recognition.data_preprocessing import load_keypoints_data_as_kp\n",
    "\n",
    "from watch_recognition.data_preprocessing import load_keypoints_data_2\n",
    "\n",
    "from watch_recognition.augmentations import set_shapes, \\\n",
    "    process_kp_data, encode_keypoints_to_mask\n",
    "from functools import partial\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "EPOCHS = 150\n",
    "image_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "        weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "        input_shape=(*image_size, 3),\n",
    "        include_top=False,\n",
    "    )\n",
    "for layer in base_model.layers:\n",
    "    if 'project_conv' in layer.name:\n",
    "        print(layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model = build_backbone(image_size)\n",
    "# base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "inputs = tf.keras.Input(shape=(*image_size, 3), )\n",
    "x = base_model(inputs)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2, strides=1,padding='same')(x)\n",
    "x = tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=3, strides=1, padding=\"same\", activation='relu'\n",
    ")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2, strides=1,padding='same')(x)\n",
    "output = tf.keras.layers.Conv2D(\n",
    "    filters=5, kernel_size=1, strides=1, padding=\"same\", activation='softmax'\n",
    ")(x)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=inputs, outputs=output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.output.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, y = load_keypoints_data_as_kp(\n",
    "    Path(\"../download_data/keypoints/train\"),\n",
    ")\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "ds_alb = dataset.map(\n",
    "        process_kp_data,\n",
    "        num_parallel_calls=AUTOTUNE,\n",
    ")\n",
    "ds_alb = ds_alb.map(\n",
    "        partial(encode_keypoints_to_mask,\n",
    "                image_size=(224, 224, 3),\n",
    "                mask_size=(14,14),\n",
    "                extent=(1,1),\n",
    "        ),\n",
    "        num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "ds_alb = ds_alb.map(\n",
    "    partial(set_shapes,\n",
    "            img_shape=(224, 224, 3),\n",
    "            target_shape=(14,14, 5)),\n",
    "    num_parallel_calls=AUTOTUNE).shuffle(8 * 32).batch(32).prefetch(AUTOTUNE)\n",
    "ds_alb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from watch_recognition.augmentations import view_image, encode_keypoints_to_mask_np\n",
    "import numpy as np\n",
    "view_image(ds_alb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_val, y_val = load_keypoints_data_as_kp(\n",
    "    Path(\"../download_data/keypoints/validation\"),\n",
    ")\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_val = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "dataset_val = dataset_val.map(\n",
    "        partial(encode_keypoints_to_mask,\n",
    "                image_size=(224, 224, 3),\n",
    "                mask_size=(14,14),\n",
    "                extent=(1,1)),\n",
    "        num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "dataset_val = dataset_val.map(\n",
    "    partial(set_shapes,\n",
    "            img_shape=(224, 224, 3),\n",
    "            target_shape=(14,14, 5)),\n",
    "    num_parallel_calls=AUTOTUNE).batch(X_val.shape[0]).prefetch(AUTOTUNE)\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "view_image(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# y[0, 0, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from watch_recognition.reports import visualize_high_loss_examples\n",
    "from watch_recognition.models import custom_focal_loss\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(3e-4) # use lower lr\n",
    "# loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-8)\n",
    "loss = custom_focal_loss\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "TYPE='keypoint'\n",
    "MODEL_NAME='efficientnetb0'\n",
    "logdir = f\"tensorboard_logs/{TYPE}/{MODEL_NAME}/run_{start.timestamp()}\"\n",
    "print(logdir)\n",
    "file_writer_distance_metrics_train = tf.summary.create_file_writer(logdir + \"/train\")\n",
    "file_writer_distance_metrics_validation = tf.summary.create_file_writer(logdir + \"/validation\")\n",
    "\n",
    "model_path = f'models/{TYPE}/{MODEL_NAME}/run_{start.timestamp()}.h5'\n",
    "model.fit(\n",
    "    ds_alb,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=logdir,\n",
    "            update_freq=\"epoch\",\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=model_path,\n",
    "                save_weights_only=False,\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.8,\n",
    "                              patience=20, min_lr=1e-6, cooldown=3),\n",
    "        # tf.keras.callbacks.EarlyStopping(\n",
    "        #     monitor=\"val_loss\",\n",
    "        #     restore_best_weights=True,\n",
    "        #     patience=15,\n",
    "        # ),\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=partial(log_scalar_metrics, X=X, y=y,\n",
    "                                 file_writer=file_writer_distance_metrics_train, model=model)),\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "                on_epoch_end=partial(visualize_high_loss_examples,\n",
    "                                     dataset=ds_alb,\n",
    "                                     file_writer=file_writer_distance_metrics_train, model=model,\n",
    "        )),\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=partial(log_scalar_metrics, X=X_val, y=y_val,\n",
    "                                 file_writer=file_writer_distance_metrics_validation, model=model)),\n",
    "\n",
    "\n",
    "    ]\n",
    ")\n",
    "elapsed = (datetime.now() - start).seconds\n",
    "print(f\"total training time: {elapsed / 60} minutes, average: {elapsed/60/EPOCHS} minutes/epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = next(iter(ds_alb))\n",
    "train_X, train_y = train_X.numpy(), train_y.numpy()\n",
    "run_on_image_debug(model, train_X[0], None, show_grid=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = Path(\"../example_data/Zrzut ekranu 2021-08-25 o 22.24.14.png\")\n",
    "test_image = tf.keras.preprocessing.image.load_img(\n",
    "    path, \"rgb\", target_size=image_size, interpolation=\"bicubic\",\n",
    ")\n",
    "test_image_np = tf.keras.preprocessing.image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_on_image_debug(model, test_image_np, show_grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_report_for_keypoints(model, X, None, show_top_n_errors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_report_for_keypoints(model, X_val, None, show_top_n_errors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model_path = \"./models/keypoint/efficientnetb0/run_1630519001.517935.h5\"\n",
    "loaded_model = tf.keras.models.load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for path in [\n",
    "    Path(\"../example_data/Zrzut ekranu 2021-08-25 o 22.24.14.png\"),\n",
    "    Path(\"../example_data/Zrzut ekranu 2021-08-25 o 22.24.24.png\"),\n",
    "    Path(\"../example_data/test-image-2.jpg\"),\n",
    "]:\n",
    "    test_image = tf.keras.preprocessing.image.load_img(\n",
    "        path, \"rgb\", target_size=image_size, interpolation=\"bicubic\",\n",
    "    )\n",
    "    test_image_np = tf.keras.preprocessing.image.img_to_array(test_image)\n",
    "\n",
    "    run_on_image_debug(loaded_model, test_image_np, show_grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_report_for_keypoints(loaded_model, X, None, show_top_n_errors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_report_for_keypoints(loaded_model, X_val, None, show_top_n_errors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for image in X_val[:10]:\n",
    "    run_on_image_debug(loaded_model, image, show_grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = loaded_model.predict(X[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for layer in range(3):\n",
    "    plt.imshow(pred[0, :,:,layer])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import  expit\n",
    "for layer in range(3):\n",
    "    output = pred[0, :,:, layer]\n",
    "    output = np.reshape(output, (14, 14))\n",
    "    plt.imshow(output)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indices = np.argmax(pred[0],axis=-1)\n",
    "for i in np.unique(indices):\n",
    "    mask = indices == i\n",
    "    plt.imshow(mask)\n",
    "    plt.show()\n",
    "# TODO add background layer and use softmax to make each output coordinate act like\n",
    "# multi class classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "view_image(ds_alb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iterator = ds_alb.take(1).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_batch, y_batch = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.metrics.categorical_crossentropy(y_batch, predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "worst_examples = np.argsort(loss.sum(axis=-1).sum(axis=-1))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for X_img in X_batch[worst_examples]:\n",
    "    plt.imshow(X_img)\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-watch-metal)",
   "language": "python",
   "name": "tf-watch-metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
